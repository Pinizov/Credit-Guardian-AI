# Legal Data Import System - Complete Usage Guide

## Overview

This comprehensive guide walks you through using the Credit Guardian AI Legal Data Import System. Whether you're setting up the system for the first time or performing routine imports, this guide covers everything you need to know.

**Target Audience**: Developers, System Administrators, Data Engineers  
**Prerequisites**: Python 3.12+, basic command line knowledge  
**Estimated Setup Time**: 15-30 minutes

---

## Table of Contents

1. [Quick Start](#quick-start)
2. [Installation](#installation)
3. [Configuration](#configuration)
4. [Basic Usage](#basic-usage)
5. [Advanced Usage](#advanced-usage)
6. [Troubleshooting](#troubleshooting)
7. [Best Practices](#best-practices)
8. [FAQ](#faq)

---

## Quick Start

Get up and running in 5 minutes:

```bash
# 1. Clone repository
cd /path/to/Credit-Guardian-AI

# 2. Create virtual environment
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate

# 3. Install dependencies
pip install -r requirements.txt

# 4. Initialize database
python -c "from database.models import Base; from sqlalchemy import create_engine; engine = create_engine('sqlite:///credit_guardian.db'); Base.metadata.create_all(engine)"

# 5. Run first import
python import_ciela_laws.py

# 6. Verify import
python check_imported_laws.py
```

**Expected Output**:
```
‚úÖ Total Legal Documents: 9
‚úÖ Total Legal Articles: 1,847
```

---

## Installation

### Prerequisites

**System Requirements**:
- Python 3.12 or higher
- 500MB free disk space (for database)
- Internet connection (for scraping)
- Operating System: Windows, macOS, or Linux

**Required Python Packages**:
```txt
requests==2.31.0
beautifulsoup4==4.12.2
lxml==4.9.3
sqlalchemy==2.0.23
```

### Step-by-Step Installation

#### 1. Python Environment Setup

**Check Python version**:
```bash
python --version
# Should output: Python 3.12.x or higher
```

**Create virtual environment**:
```bash
# Navigate to project directory
cd /path/to/Credit-Guardian-AI

# Create virtual environment
python -m venv .venv

# Activate virtual environment
# On Linux/Mac:
source .venv/bin/activate

# On Windows (PowerShell):
.\.venv\Scripts\Activate.ps1

# On Windows (CMD):
.venv\Scripts\activate.bat
```

#### 2. Install Dependencies

**Install from requirements.txt**:
```bash
pip install --upgrade pip
pip install -r requirements.txt
```

**Verify installation**:
```bash
python -c "import requests, bs4, sqlalchemy; print('‚úÖ All dependencies installed')"
```

#### 3. Database Setup

**Option A: Automatic Setup** (Recommended)

```bash
# Run database initialization
python database/init_all_tables.py
```

**Option B: Manual Setup**

```python
# Create database and tables
from database.models import Base
from sqlalchemy import create_engine

engine = create_engine('sqlite:///credit_guardian.db')
Base.metadata.create_all(engine)
print("‚úÖ Database created successfully")
```

**Verify database**:
```bash
# Check that database file was created
ls -lh credit_guardian.db

# On Windows:
dir credit_guardian.db
```

Expected output:
```
-rw-r--r-- 1 user user 20K Nov 24 10:00 credit_guardian.db
```

---

## Configuration

### Database Configuration

**Default Configuration**:
```python
DB_PATH = 'credit_guardian.db'  # SQLite database file
DB_ECHO = False                  # Set to True for SQL logging
```

**Custom Database Location**:
```python
# In your import script
importer = CielaLawImporter(
    db_path='/custom/path/to/database.db',
    csv_path='ciela.csv'
)
```

**Environment Variables**:
```bash
# Set custom database path
export CREDIT_GUARDIAN_DB_PATH=/data/credit_guardian.db

# Use in Python
import os
db_path = os.getenv('CREDIT_GUARDIAN_DB_PATH', 'credit_guardian.db')
```

### Scraper Configuration

**Rate Limiting**:
```python
# Default: 2 seconds between requests
scraper = CielaNetScraper(delay=2.0)

# More aggressive (use with caution)
scraper = CielaNetScraper(delay=1.0)

# More polite (recommended for production)
scraper = CielaNetScraper(delay=3.0)
```

**Timeout Settings**:
```python
# Modify in scraper class
class CielaNetScraper:
    def __init__(self, delay=2.0, timeout=15):
        self.timeout = timeout  # seconds
        
    def fetch(self, url):
        response = self.session.get(url, timeout=self.timeout)
```

**User Agent Configuration**:
```python
# Default user agent
USER_AGENT = 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'

# Custom user agent
scraper = CielaNetScraper()
scraper.session.headers['User-Agent'] = 'YourCustomUserAgent/1.0'
```

### Logging Configuration

**Basic Logging**:
```python
import logging

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('import.log'),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)
```

**Advanced Logging**:
```python
# Separate log files for different components
import logging.config

LOGGING_CONFIG = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'standard': {
            'format': '%(asctime)s [%(levelname)s] %(name)s: %(message)s'
        },
    },
    'handlers': {
        'scraper': {
            'class': 'logging.FileHandler',
            'filename': 'logs/scraper.log',
            'formatter': 'standard',
        },
        'database': {
            'class': 'logging.FileHandler',
            'filename': 'logs/database.log',
            'formatter': 'standard',
        },
    },
    'loggers': {
        'scrapers': {
            'handlers': ['scraper'],
            'level': 'INFO',
        },
        'database': {
            'handlers': ['database'],
            'level': 'INFO',
        },
    }
}

logging.config.dictConfig(LOGGING_CONFIG)
```

---

## Basic Usage

### Importing Laws from Ciela.net

#### Import Priority Laws Only

The fastest way to get essential consumer credit laws:

```python
from import_ciela_laws import CielaLawImporter
import time

# Initialize importer
importer = CielaLawImporter(
    db_path='credit_guardian.db',
    csv_path='ciela.csv'
)

# Import only priority laws
print("Importing priority consumer credit laws...")
importer.import_all_priority_laws()

print("‚úÖ Priority laws imported")
```

**Priority Laws Imported**:
1. –ó–∞–∫–æ–Ω –∑–∞ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª—Å–∫–∏—è –∫—Ä–µ–¥–∏—Ç (Consumer Credit Law)
2. –ó–∞–∫–æ–Ω –∑–∞ –∫—Ä–µ–¥–∏—Ç–∏—Ç–µ –∑–∞ –Ω–µ–¥–≤–∏–∂–∏–º–∏ –∏–º–æ—Ç–∏ (Real Estate Credit Law)
3. –ó–∞–∫–æ–Ω –∑–∞ –∑–∞—â–∏—Ç–∞ –Ω–∞ –ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª–∏—Ç–µ (Consumer Protection Law)
4. –ó–∞–∫–æ–Ω –∑–∞ –∫—Ä–µ–¥–∏—Ç–Ω–∏—Ç–µ –∏–Ω—Å—Ç–∏—Ç—É—Ü–∏–∏ (Credit Institutions Law)
5. –ó–∞–∫–æ–Ω –∑–∞ –∑–∞–¥—ä–ª–∂–µ–Ω–∏—è—Ç–∞ –∏ –¥–æ–≥–æ–≤–æ—Ä–∏—Ç–µ (Obligations and Contracts Law)
6. –ó–∞–∫–æ–Ω –∑–∞ –Ω–µ—Å—ä—Å—Ç–æ—è—Ç–µ–ª–Ω–æ—Å—Ç –Ω–∞ —Ñ–∏–∑–∏—á–µ—Å–∫–∏—Ç–µ –ª–∏—Ü–∞ (Personal Insolvency Law)
7. –ó–∞–∫–æ–Ω –∑–∞ –∏–ø–æ—Ç–µ—á–Ω–∏—Ç–µ –æ–±–ª–∏–≥–∞—Ü–∏–∏ (Mortgage Bonds Law)
8. –ó–∞–∫–æ–Ω –∑–∞ –∑–∞—â–∏—Ç–∞ –æ—Ç –¥–∏—Å–∫—Ä–∏–º–∏–Ω–∞—Ü–∏—è (Anti-Discrimination Law)
9. –ó–∞–∫–æ–Ω –∑–∞ –∫–æ–º–∏—Å–∏—è—Ç–∞ –∑–∞ —Ñ–∏–Ω–∞–Ω—Å–æ–≤ –Ω–∞–¥–∑–æ—Ä (Financial Supervision Law)

#### Import All Laws

For comprehensive legal coverage:

```python
from import_ciela_laws import CielaLawImporter

importer = CielaLawImporter()

# Read all laws from CSV
laws = importer.read_csv()
print(f"Found {len(laws)} laws in CSV")

# Import all laws
for i, law in enumerate(laws, 1):
    print(f"\n[{i}/{len(laws)}] Importing: {law['title']}")
    
    try:
        result = importer.import_law(law)
        if result:
            print(f"  ‚úÖ Success - {result.id}")
        else:
            print(f"  ‚è≠Ô∏è  Skipped (already exists)")
    except Exception as e:
        print(f"  ‚ùå Error: {e}")
    
    time.sleep(2)  # Rate limiting

print("\n‚úÖ Import complete")
```

**Expected Duration**: 8-15 minutes for ~150 laws

#### Import Specific Law by Title

Import a single law by searching for it:

```python
from import_ciela_laws import CielaLawImporter

importer = CielaLawImporter()
laws = importer.read_csv()

# Find specific law
target_title = "–ó–ê–ö–û–ù –ó–ê –ü–û–¢–†–ï–ë–ò–¢–ï–õ–°–ö–ò–Ø –ö–†–ï–î–ò–¢"
law = next((l for l in laws if target_title in l['title']), None)

if law:
    print(f"Importing: {law['title']}")
    result = importer.import_law(law, priority=True)
    print(f"‚úÖ Imported with ID: {result.id}")
else:
    print(f"‚ùå Law not found: {target_title}")
```

### Importing Legal Codexes

Import major Bulgarian codes:

```python
from import_codexes import CodexImporter
import time

importer = CodexImporter(db_path='credit_guardian.db')

print("Importing Bulgarian Legal Codes...")
print("=" * 60)

# Import all codexes
for title, url in importer.CODEX_LIST:
    print(f"\nüìö {title[:60]}...")
    
    try:
        # Fetch content
        content = importer.fetch_content(url)
        
        if content:
            # Extract articles
            articles = importer.extract_articles(content)
            
            # Save to database
            doc_id = importer.save_document(title, content, url)
            
            for article in articles:
                importer.save_article(doc_id, article)
            
            print(f"   ‚úÖ Saved {len(articles)} articles")
        else:
            print(f"   ‚ùå Failed to fetch content")
            
    except Exception as e:
        print(f"   ‚ùå Error: {e}")
    
    time.sleep(3)  # Rate limiting

print("\n‚úÖ Codex import complete")
```

### Importing the Constitution

Import the Bulgarian Constitution:

```python
from import_constitution import ConstitutionImporter

importer = ConstitutionImporter(db_path='credit_guardian.db')

print("Importing Bulgarian Constitution...")

# Fetch and import
result = importer.fetch_constitution()

if result:
    print(f"Title: {result['title']}")
    print(f"Articles: {len(result['articles'])}")
    
    # Save to database
    success = importer.save_to_database(result)
    
    if success:
        print("‚úÖ Constitution imported successfully")
    else:
        print("‚ùå Failed to save to database")
else:
    print("‚ùå Failed to fetch constitution")
```

### Verifying Imports

Check what has been imported:

```bash
# Run verification script
python check_imported_laws.py
```

**Or programmatically**:

```python
from database.models import Session
from database.legal_models import LegalDocument, LegalArticle

session = Session()

# Get statistics
doc_count = session.query(LegalDocument).count()
article_count = session.query(LegalArticle).count()

print(f"üìä Database Statistics:")
print(f"  Documents: {doc_count}")
print(f"  Articles: {article_count}")

# List all documents
docs = session.query(LegalDocument).all()
for doc in docs:
    article_count = session.query(LegalArticle).filter_by(
        document_id=doc.id
    ).count()
    print(f"\nüìÑ {doc.title}")
    print(f"   Articles: {article_count}")
    print(f"   Type: {doc.document_type}")

session.close()
```

---

## Advanced Usage

### Selective Import with Filters

Import only specific types of documents:

```python
from import_ciela_laws import CielaLawImporter

importer = CielaLawImporter()
laws = importer.read_csv()

# Filter by keywords
keywords = ['–∫—Ä–µ–¥–∏—Ç', '–ø–æ—Ç—Ä–µ–±–∏—Ç–µ–ª', '–±–∞–Ω–∫']
filtered_laws = [
    law for law in laws 
    if any(keyword in law['title'].lower() for keyword in keywords)
]

print(f"Found {len(filtered_laws)} laws matching keywords")

# Import filtered laws
for law in filtered_laws:
    importer.import_law(law)
    time.sleep(2)
```

### Parallel Import

Speed up imports with multiprocessing:

```python
from multiprocessing import Pool
from import_ciela_laws import CielaLawImporter
import time

def import_single_law(law_data):
    """Import a single law (worker function)."""
    importer = CielaLawImporter()
    try:
        result = importer.import_law(law_data)
        return {'success': True, 'title': law_data['title']}
    except Exception as e:
        return {'success': False, 'title': law_data['title'], 'error': str(e)}

def parallel_import(laws, workers=4):
    """Import laws in parallel."""
    print(f"Starting parallel import with {workers} workers...")
    
    with Pool(processes=workers) as pool:
        results = pool.map(import_single_law, laws)
    
    # Summarize results
    successful = sum(1 for r in results if r['success'])
    failed = len(results) - successful
    
    print(f"\nüìä Results:")
    print(f"  ‚úÖ Success: {successful}")
    print(f"  ‚ùå Failed: {failed}")
    
    # Show failures
    if failed > 0:
        print(f"\nFailed imports:")
        for r in results:
            if not r['success']:
                print(f"  ‚Ä¢ {r['title']}: {r.get('error', 'Unknown error')}")

# Usage
importer = CielaLawImporter()
laws = importer.read_csv()

# Import with 4 parallel workers
parallel_import(laws[:20], workers=4)  # Test with first 20 laws
```

**Note**: Be respectful of server load when using parallel requests.

### Incremental Updates

Check for and import only new or updated content:

```python
from database.models import Session
from database.legal_models import LegalDocument
from import_ciela_laws import CielaLawImporter
import time

def incremental_import():
    """Import only new or updated laws."""
    
    importer = CielaLawImporter()
    session = Session()
    
    # Get existing law titles
    existing_titles = {
        doc.title for doc in session.query(LegalDocument).all()
    }
    
    # Read all laws from CSV
    all_laws = importer.read_csv()
    
    # Filter to only new laws
    new_laws = [
        law for law in all_laws 
        if law['title'] not in existing_titles
    ]
    
    print(f"Found {len(new_laws)} new laws to import")
    
    # Import new laws
    for law in new_laws:
        print(f"Importing: {law['title']}")
        importer.import_law(law)
        time.sleep(2)
    
    session.close()
    print("‚úÖ Incremental import complete")

# Run incremental import
incremental_import()
```

### Export to JSON

Export legal data for backup or analysis:

```python
import json
from database.models import Session
from database.legal_models import LegalDocument, LegalArticle
from datetime import datetime

def export_to_json(output_file='legal_data_export.json'):
    """Export all legal data to JSON."""
    
    session = Session()
    
    export_data = {
        'export_date': datetime.now().isoformat(),
        'version': '1.0',
        'documents': []
    }
    
    docs = session.query(LegalDocument).all()
    
    print(f"Exporting {len(docs)} documents...")
    
    for doc in docs:
        # Get articles for this document
        articles = session.query(LegalArticle).filter_by(
            document_id=doc.id
        ).all()
        
        doc_data = {
            'id': doc.id,
            'title': doc.title,
            'document_type': doc.document_type,
            'document_number': doc.document_number,
            'promulgation_date': doc.promulgation_date.isoformat() if doc.promulgation_date else None,
            'effective_date': doc.effective_date.isoformat() if doc.effective_date else None,
            'source_url': doc.source_url,
            'is_active': doc.is_active,
            'created_at': doc.created_at.isoformat(),
            'article_count': len(articles),
            'articles': [
                {
                    'id': art.id,
                    'article_number': art.article_number,
                    'title': art.title,
                    'content': art.content,
                    'chapter': art.chapter,
                }
                for art in articles
            ]
        }
        
        export_data['documents'].append(doc_data)
    
    # Write to file
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(export_data, f, ensure_ascii=False, indent=2)
    
    session.close()
    
    print(f"‚úÖ Exported to {output_file}")
    print(f"   Documents: {len(docs)}")
    print(f"   Total articles: {sum(len(d['articles']) for d in export_data['documents'])}")

# Usage
export_to_json('backup_' + datetime.now().strftime('%Y%m%d') + '.json')
```

### Custom Article Extraction

Implement custom extraction logic:

```python
import re

def extract_articles_custom(text, document_type='law'):
    """
    Custom article extraction with enhanced pattern recognition.
    """
    articles = []
    
    # Define patterns based on document type
    if document_type == 'law':
        pattern = r'(?:–ß–ª\.|–ß–ª–µ–Ω)\s*(\d+[–∞-—è]?)\.'
    elif document_type == 'constitution':
        pattern = r'–ß–ª\.\s*(\d+)\.'
    elif document_type == 'code':
        pattern = r'(?:–ß–ª\.|–ß–ª–µ–Ω)\s*(\d+[–∞-—è]?)\.'
    else:
        pattern = r'(?:–ß–ª\.|–ß–ª–µ–Ω|¬ß)\s*(\d+[–∞-—è]?)\.'
    
    # Find all article markers
    matches = list(re.finditer(pattern, text, re.MULTILINE))
    
    for i, match in enumerate(matches):
        article_number = match.group(1)
        start = match.start()
        
        # Determine end position
        if i < len(matches) - 1:
            end = matches[i + 1].start()
        else:
            # Last article - take remaining text or max 5000 chars
            end = min(start + 5000, len(text))
        
        # Extract content
        content = text[start:end].strip()
        
        # Extract title if present
        title_match = re.search(
            r'(?:–ß–ª\.|–ß–ª–µ–Ω)\s*\d+[–∞-—è]?\.\s*(?:\([^\)]+\))?\s*([^\n]+)',
            content
        )
        title = title_match.group(1) if title_match else None
        
        # Only include substantial articles
        if len(content) > 50:
            articles.append({
                'number': article_number,
                'title': title,
                'content': content,
                'position': i + 1
            })
    
    return articles

# Usage
with open('law_text.txt', 'r', encoding='utf-8') as f:
    text = f.read()

articles = extract_articles_custom(text, document_type='law')
print(f"Extracted {len(articles)} articles")
```

---

## Troubleshooting

### Common Issues

#### Issue 1: ImportError - Missing Dependencies

**Symptom**:
```
ImportError: No module named 'requests'
```

**Solution**:
```bash
# Ensure virtual environment is activated
source .venv/bin/activate  # Linux/Mac
.\.venv\Scripts\activate   # Windows

# Install dependencies
pip install -r requirements.txt

# Verify installation
pip list | grep requests
```

#### Issue 2: Database Locked

**Symptom**:
```
sqlite3.OperationalError: database is locked
```

**Solution**:
```python
# Enable WAL mode for better concurrency
import sqlite3
conn = sqlite3.connect('credit_guardian.db')
conn.execute('PRAGMA journal_mode=WAL;')
conn.close()

# Or in SQLAlchemy
from sqlalchemy import create_engine, event

engine = create_engine('sqlite:///credit_guardian.db')

@event.listens_for(engine, "connect")
def set_sqlite_pragma(dbapi_conn, connection_record):
    cursor = dbapi_conn.cursor()
    cursor.execute("PRAGMA journal_mode=WAL")
    cursor.close()
```

#### Issue 3: Timeout Errors

**Symptom**:
```
requests.exceptions.Timeout: HTTPConnectionPool
```

**Solution**:
```python
# Increase timeout
scraper = CielaNetScraper(delay=2.0)
scraper.session.timeout = 30  # Increase to 30 seconds

# Or implement retry logic
from requests.adapters import HTTPAdapter
from requests.packages.urllib3.util.retry import Retry

retry_strategy = Retry(
    total=3,
    backoff_factor=2,
    status_forcelist=[429, 500, 502, 503, 504]
)

adapter = HTTPAdapter(max_retries=retry_strategy)
scraper.session.mount("http://", adapter)
scraper.session.mount("https://", adapter)
```

#### Issue 4: Encoding Errors

**Symptom**:
```
UnicodeDecodeError: 'utf-8' codec can't decode byte
```

**Solution**:
```python
# Force UTF-8 encoding
response = requests.get(url)
response.encoding = 'utf-8'
text = response.text

# Or handle different encodings
from charset_normalizer import detect

response = requests.get(url)
encoding = detect(response.content)['encoding']
text = response.content.decode(encoding)
```

#### Issue 5: HTML Parsing Issues

**Symptom**:
```
No content found / Empty articles list
```

**Solution**:
```python
# Debug HTML structure
soup = BeautifulSoup(html, 'lxml')
print(soup.prettify()[:500])  # Inspect first 500 chars

# Try different selectors
content = (
    soup.find('div', class_='law-content') or
    soup.find('div', class_='document-content') or
    soup.find('article') or
    soup.find('main') or
    soup.find('body')
)

# Save HTML for inspection
with open('debug.html', 'w', encoding='utf-8') as f:
    f.write(str(soup))
```

### Performance Issues

#### Slow Import Speed

**Diagnosis**:
```python
import time

# Add timing to identify bottleneck
start = time.time()
content = scraper.fetch_content(url)
fetch_time = time.time() - start

start = time.time()
articles = extract_articles(content)
extract_time = time.time() - start

start = time.time()
save_to_database(doc, articles)
db_time = time.time() - start

print(f"Fetch: {fetch_time:.2f}s")
print(f"Extract: {extract_time:.2f}s")
print(f"Database: {db_time:.2f}s")
```

**Solutions**:
```python
# 1. Reduce rate limiting (if appropriate)
scraper = CielaNetScraper(delay=1.0)

# 2. Use batch database operations
session.add_all(articles)  # Add all at once
session.commit()           # Single commit

# 3. Enable SQLite optimizations
conn.execute('PRAGMA synchronous=NORMAL;')
conn.execute('PRAGMA cache_size=10000;')
```

#### High Memory Usage

**Solution**:
```python
# Process in batches
def import_in_batches(laws, batch_size=10):
    for i in range(0, len(laws), batch_size):
        batch = laws[i:i+batch_size]
        
        for law in batch:
            importer.import_law(law)
        
        # Clear session to free memory
        session.expunge_all()
        
        print(f"Processed batch {i//batch_size + 1}")

import_in_batches(laws, batch_size=10)
```

### Debugging

**Enable Debug Logging**:
```python
import logging

logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)

# Now run your import
```

**Inspect Database**:
```bash
# Open SQLite database
sqlite3 credit_guardian.db

# Run queries
.tables
SELECT COUNT(*) FROM legal_documents;
SELECT COUNT(*) FROM legal_articles;
SELECT title, COUNT(*) as articles 
FROM legal_documents d 
LEFT JOIN legal_articles a ON d.id = a.document_id 
GROUP BY d.id;
```

---

## Best Practices

### 1. Rate Limiting

**Always respect server resources**:
```python
# Minimum 2 seconds between requests
scraper = CielaNetScraper(delay=2.0)

# For production, use 3+ seconds
scraper = CielaNetScraper(delay=3.0)
```

### 2. Error Handling

**Implement comprehensive error handling**:
```python
def safe_import(law_data):
    """Import with error handling."""
    try:
        result = importer.import_law(law_data)
        return {'success': True, 'result': result}
    except requests.exceptions.Timeout:
        return {'success': False, 'error': 'timeout'}
    except requests.exceptions.ConnectionError:
        return {'success': False, 'error': 'connection'}
    except Exception as e:
        return {'success': False, 'error': str(e)}
```

### 3. Validation

**Validate data before saving**:
```python
def validate_document(doc_data):
    """Validate document data."""
    if not doc_data.get('title'):
        raise ValueError("Title is required")
    
    if not doc_data.get('content'):
        raise ValueError("Content is required")
    
    if len(doc_data['content']) < 100:
        raise ValueError("Content too short")
    
    return True
```

### 4. Backup

**Regular backups**:
```bash
# Backup before major imports
cp credit_guardian.db credit_guardian_backup_$(date +%Y%m%d).db

# Or use SQLite backup
sqlite3 credit_guardian.db ".backup 'backup.db'"
```

### 5. Monitoring

**Track import progress**:
```python
from tqdm import tqdm

for law in tqdm(laws, desc="Importing"):
    importer.import_law(law)
    time.sleep(2)
```

---

## FAQ

### Q: How long does a full import take?

**A**: Approximately 8-15 minutes for 150 laws, depending on:
- Network speed
- Server response time
- Rate limiting delay
- System performance

### Q: Can I interrupt and resume an import?

**A**: Yes. The system automatically skips already-imported laws:
```python
# Interrupted imports can be safely resumed
importer.import_all_laws()  # Will skip existing laws
```

### Q: How much disk space is needed?

**A**: For complete Bulgarian legal corpus:
- Database: 80-120 MB
- Logs: 10-50 MB
- Total: ~150 MB

### Q: Can I import from multiple sources simultaneously?

**A**: Yes, but use different database sessions:
```python
# Thread-safe import
from threading import Thread

def import_source1():
    importer = CielaLawImporter()  # New session
    importer.import_all_laws()

def import_source2():
    importer = CodexImporter()  # New session
    importer.import_all_codexes()

t1 = Thread(target=import_source1)
t2 = Thread(target=import_source2)
t1.start()
t2.start()
t1.join()
t2.join()
```

### Q: How do I update existing laws?

**A**: Delete and re-import, or implement update logic:
```python
# Option 1: Delete and re-import
session.query(LegalDocument).filter_by(title=title).delete()
importer.import_law(law_data)

# Option 2: Update in place
doc = session.query(LegalDocument).filter_by(title=title).first()
if doc:
    doc.full_text = new_content
    # Update articles...
    session.commit()
```

### Q: Can I use PostgreSQL instead of SQLite?

**A**: Yes, change the database URL:
```python
# PostgreSQL
engine = create_engine('postgresql://user:pass@localhost/credit_guardian')

# MySQL
engine = create_engine('mysql+pymysql://user:pass@localhost/credit_guardian')
```

---

## Support

For issues, questions, or contributions:
- Check [FEATURES_DEMONSTRATION.md](FEATURES_DEMONSTRATION.md) for examples
- See [IMPLEMENTATION_SUMMARY.md](IMPLEMENTATION_SUMMARY.md) for technical details
- Refer to [QUICK_IMPORT_REFERENCE.md](QUICK_IMPORT_REFERENCE.md) for quick commands

---

**Last Updated**: November 2024  
**Version**: 1.0  
**Status**: Production Ready ‚úÖ
