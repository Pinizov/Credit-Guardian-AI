# Credit Guardian AI - Docker Compose Configuration
# Includes: PostgreSQL, Ollama, FastAPI, Frontend, System Monitor
# Note: version field removed (obsolete in newer Docker Compose)

services:
  # =============================================================================
  # OLLAMA - Local LLM Server (FREE!)
  # =============================================================================
  ollama:
    image: ollama/ollama:latest
    container_name: cg_ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    volumes:
      - ollama_data:/root/.ollama
    # GPU support (optional - remove if no GPU or WSL without GPU)
    # Uncomment below if you have NVIDIA GPU with proper drivers
    # deploy:
    #   resources:
    #     reservations:
    #       devices:
    #         - driver: nvidia
    #           count: all
    #           capabilities: [gpu]
    # Ollama works perfectly on CPU too - just slower

  # Pull the model after Ollama starts
  ollama-pull:
    image: ollama/ollama:latest
    container_name: cg_ollama_pull
    depends_on:
      - ollama
    restart: "no"
    entrypoint: ["/bin/sh", "-c"]
    command:
      - |
        sleep 10
        ollama pull llama3.2
        echo "Model llama3.2 ready!"

  # =============================================================================
  # DATABASE - PostgreSQL
  # =============================================================================
  db:
    image: postgres:15-alpine
    container_name: cg_postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: credit_guardian
      POSTGRES_USER: cg_user
      POSTGRES_PASSWORD: cg_pass
    ports:
      - "5432:5432"
    volumes:
      - pgdata:/var/lib/postgresql/data
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U cg_user -d credit_guardian"]
      interval: 10s
      timeout: 5s
      retries: 5

  # =============================================================================
  # BACKEND API - FastAPI with Reliability Features
  # =============================================================================
  api:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: cg_api
    restart: unless-stopped
    depends_on:
      db:
        condition: service_healthy
      ollama:
        condition: service_started
    environment:
      # Database Configuration with Connection Pooling
      DATABASE_URL: postgresql+psycopg2://cg_user:cg_pass@db:5432/credit_guardian
      DB_POOL_SIZE: 20
      DB_POOL_RECYCLE: 3600
      DB_POOL_TIMEOUT: 30
      DB_MAX_RETRIES: 3
      DB_RETRY_DELAY: 1.0
      DB_ECHO: "false"
      
      # AI Provider Configuration (Ollama recommended)
      AI_PROVIDER: ollama
      OLLAMA_URL: http://ollama:11434
      OLLAMA_MODEL: llama3.2
      
      # Server Configuration
      PORT: 8000
      HOST: 0.0.0.0
      MAX_RESTARTS: 10
      RESTART_DELAY: 5
      
      # Logging
      LOG_LEVEL: INFO
    ports:
      - "8080:8000"
    volumes:
      - ./uploads:/app/uploads
      - ./server.log:/app/server.log
      # Mount code directory for development (remove :ro for write access if needed)
      - .:/app
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8000/readiness"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 40s

  # =============================================================================
  # DATABASE MIGRATIONS
  # =============================================================================
  migrate:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: cg_migrate
    depends_on:
      db:
        condition: service_healthy
    environment:
      DATABASE_URL: postgresql+psycopg2://cg_user:cg_pass@db:5432/credit_guardian
    entrypoint: ["sh", "-c", "alembic upgrade head && python database/seed_db.py"]
    restart: "no"

  # =============================================================================
  # FRONTEND - Static HTML with Nginx
  # =============================================================================
  frontend:
    image: nginx:alpine
    container_name: cg_frontend
    restart: unless-stopped
    depends_on:
      - api
    ports:
      - "3000:80"
    volumes:
      - ./frontend/index.html:/usr/share/nginx/html/index.html:ro
      - ./frontend/nginx.conf:/etc/nginx/conf.d/default.conf:ro
      # Ensure proper file synchronization
    environment:
      - NGINX_ENVSUBST_OUTPUT_DIR=/etc/nginx
    healthcheck:
      test: ["CMD-SHELL", "wget --quiet --tries=1 --spider http://localhost/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 10s

  # =============================================================================
  # SYSTEM MONITOR (Optional - for production monitoring)
  # =============================================================================
  monitor:
    build:
      context: .
      dockerfile: Dockerfile
    container_name: cg_monitor
    restart: unless-stopped
    depends_on:
      - api
      - db
      - ollama
    environment:
      API_URL: http://api:8000
      OLLAMA_URL: http://ollama:11434
      HEALTH_CHECK_INTERVAL: 30
      MAX_FAILURES: 3
      LOG_DIR: /app/logs
    command: ["python", "system_monitor.py"]
    volumes:
      - ./logs:/app/logs

volumes:
  pgdata:
  ollama_data:
